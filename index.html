<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/html">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <title>MUCG Workshop @ ACM 2025</title>
  <meta name="description" content="">
  <meta name="keywords" content="">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Fonts -->
  <link href="https://fonts.googleapis.com" rel="preconnect">
  <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300;0,400;0,500;0,600;0,700;0,800;1,300;1,400;1,500;1,600;1,700;1,800&family=Montserrat:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&family=Poppins:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Main CSS File -->
  <link href="assets/css/main.css" rel="stylesheet">

</head>

<body class="index-page">

  <header id="header" class="header d-flex align-items-center fixed-top">
    <div class="container-fluid position-relative d-flex align-items-center justify-content-between" style="height: 20px">

      <a href="./" class="logo d-flex align-items-center me-auto me-xl-0">
        <h1 class="sitename">MUCG 2025</h1>
      </a>

      <nav id="navmenu" class="navmenu">
        <ul>
          <li><a href="./" class="active">Overview</a></li>
          <li><a href="./#date">Important Dates</a></li>
          <li><a href="./#call">Call for Papers</a></li>
          <li><a href="./#keynote">Keynote Speakers</a></li>
          <li><a href="./#schedule">Schedule</a></li>
          <li><a href="./#task">Shared Tasks</a></li>
          <li><a href="./#organizer">Organizers</a></li>
          <li><a href="./#committee">Program Committee</a></li>
          <li><a href="./#contact">Contact</a></li>
        </ul>
        <i class="mobile-nav-toggle d-xl-none bi bi-list"></i>
      </nav>

    </div>
  </header>

  <main class="main">

    <!-- Hero Section -->
    <section id="hero" class="hero section dark-background">

      <img src="assets/img/Vienna-crop2.jpeg" alt="" data-aos="fade-in">

      <div class="container">
        <div class="row">
          <div class="col-lg-10" style="margin-left: 50px;margin-bottom: -50px;">
            <h1 data-aos="fade-up" data-aos-delay="100">MUCG @ ACM 2025</h1>
            <br>
            <h3 data-aos="fade-up" data-aos-delay="100">The 1st International Workshop on MLLM for Unified Comprehension and Generation</h3>
            <br>
            <p data-aos="fade-up" data-aos-delay="200">Dublin, Ireland</p>
            <p data-aos="fade-up" data-aos-delay="200">October 27-October 31, 2025</p>
          </div>
        </div>
      </div>

    </section><!-- /Hero Section -->

    <!-- About Section -->
    <section id="about" class="about section light-background">

      <div class="container" data-aos="fade-up" data-aos-delay="100" style="    margin-top: -30px;">
        <div class="row align-items-xl-center gy-2">

          <div class="content">
            <h3 style=" margin-bottom: 30px;">Workshop Introduction</h3>
            <p align="justify">As Multimodal Large Language Models (MLLMs) continue to advance, there is a growing need to bridge the gap between their comprehension and generation capabilities within unified frameworks. This workshop <b>MLLM for Unified Comprehension and Generation (MUCG)</b>  aims to explore and address the fundamental challenges in developing truly integrated MLLMs that can seamlessly understand and create multimodal content. We focus on three interconnected areas:</p>
            <ul>
              <li>Sophisticated multimodal comprehension, targeting robust understanding of complex visual content and semantic relationships.</li>
              <li>Controllable content generation, addressing challenges in high-fidelity synthesis and cross-modal consistency.</li>
              <li>Unified frameworks that enable semantic alignment between understanding and generation tasks.</li>
            </ul>
            <p align="justify">Unlike previous approaches that treat these capabilities separately, our workshop specifically targets their integration through MLLMs, fostering focused discussions on shared architectures, bidirectional knowledge transfer, and end-to-end training strategies.</p>
            
            <!-- Background & Motivation Section -->
            <h4 style="margin-top: 40px; margin-bottom: 20px;">Background & Motivation</h4>
            <p align="justify">Large Language Models (LLMs) have fundamentally transformed natural language processing, establishing new performance standards across various text-based tasks. The inherent multimodal nature of real-world information necessitates a unified approach that can seamlessly process and generate multimodal content. This requirement has driven the rapid development of Multimodal LLMs (MLLMs), which leverage large-scale pre-training and sophisticated architectures to enable integrated cross-modal reasoning.</p>

            <p align="justify">In the field of MLLMs, there have been three sub-venues that have subsequently undergone extensive research attention. Initially, researchers built MLLMs which have demonstrated strong capabilities in basic visual understanding tasks like processing single/multiple images and understanding short videos. Advanced functionalities like 3D scene comprehension and multi-view analysis have also shown promising results. However, significant challenges remain in several core aspects: (1) achieving robust understanding of long-form video content, (2) extracting fine-grained semantic information from complex visual scenes, (3) detecting intricate visual relationships with limited supervision, and (4) handling open-domain visual question answering. These challenges are particularly pronounced in real-world scenarios requiring sophisticated visual reasoning and temporal-spatial understanding.</p>

            <p align="justify">Later, more advances have enabled high-quality synthesis in specific domains, including text-to-image generation, controlled visual editing, and basic video synthesis. Notable progress has been made in areas like layout-controlled generation and cross-modal style transfer. However, several fundamental challenges persist: (1) ensuring semantic fidelity in complex visual generation, (2) maintaining consistency across multi-image generation tasks, (3) synthesizing realistic motion sequences in videos, and (4) creating contextually appropriate visual narratives. The field continues to face difficulties in achieving high-quality, controllable generation, particularly for dynamic and interactive content.</p>

            <p align="justify">The ultimate goal of MLLMs is to achieve seamless integration of comprehension and generation capabilities within a unified framework, which is also the recent hot focus of MLLMs. Building the unified architecture of MLLMs is also the key research direction in the coming future in the MLLM venue. While initial attempts have shown promise, creating truly integrated systems remains a fundamental challenge. Current approaches have made progress in joint vision-language modeling and shared representation learning, but several critical barriers need to be addressed: (1) designing efficient architectures that enable fluid transitions between understanding and generation, (2) developing bidirectional knowledge transfer mechanisms between comprehension and generation tasks, (3) achieving robust vision-language alignment that serves both understanding and creation, and (4) implementing scalable end-to-end training strategies for unified models. The development of instruction-tuned models that can seamlessly switch between comprehension and generation modes remains crucial for real-world applications.</p>

            <p align="justify">This specialized venue creates unique opportunities for focused discussions on MLLM integration challenges, complementing the broader multimedia research agenda of the main conference. By bringing together experts in this trending topic, we seek to catalyze breakthroughs in developing truly unified foundation multimodal systems that can both understand and create content seamlessly.</p>
          </div>
        </div>
      </div>
    </section>

    <!-- Target Audience Section -->
    <section id="target" class="target section light-background">
      <div class="container" data-aos="fade-up" data-aos-delay="100">
        <h3>Target Audience</h3>
        <ul>
          <li><strong>Academic researchers</strong>, including professors and graduate students working on multimodal intelligence, multimedia analysis, and cross-modal learning</li>
          <li><strong>Industry practitioners</strong>, specifically data scientists and engineers developing multimodal applications and systems</li>
          <li><strong>Broader participants</strong> from startups, government agencies, and interdisciplinary domains interested in latest multimodal AI advances</li>
        </ul>
      </div>
    </section>

    <!-- Topics Section -->
    <section id="topics" class="topics section light-background">
      <div class="container" data-aos="fade-up" data-aos-delay="100">
        <h3>Topics</h3>
        <ul>
          <li>MLLM for Multimodal Comprehension & Reasoning</li>
          <li>MLLM for Multimodal Content Generation</li>
          <li>Unified MLLM Understanding and Generation</li>
        </ul>
      </div>
    </section>

    <!-- Activities Section -->
    <section id="activities" class="activities section light-background">
      <div class="container" data-aos="fade-up" data-aos-delay="100">
        <h3>Activities</h3>
        <p>We plan to hold a hybrid format of workshop, i.e., both onsite and online. For the onsite type, at least three organizers will attend in person to host the workshop. The workshop will include two major activities, the invited keynotes, and the paper presentations. We will invite keynote presentations, followed by accepted workshop presentations.</p>
      </div>
    </section>

    <!-- Paper Submission Section -->
    <section id="paper" class="paper section light-background">
      <div class="container" data-aos="fade-up" data-aos-delay="100">
        <h3>Paper Submission</h3>
        <p>In this workshop, we welcome two types of submissions, all of which should relate to the topics and themes as listed in Section 3:</p>
        <ul>
          <li>Technical, position or perspective papers (up to 8 pages in length, plus unlimited pages for references)</li>
          <li>Featured papers (title and abstract of the paper, plus the original paper up to 8 pages)</li>
          <li>Demonstration papers (up to 4 pages in length, plus unlimited pages for references)</li>
        </ul>
        <p>Submissions should be double-blind, written in English, and formatted according to the current ACM two-column conference format.</p>
      </div>
    </section>

    <!-- Organizers Section -->
    <section id="organizer" class="organizer section light-background">
      <div class="container" data-aos="fade-up" data-aos-delay="100">
        <h3>Organizers</h3>
        <div class="row">
          <div class="col-md-4">
            <div class="organizer-info">
              <h4>Jiayi Ji</h4>
              <p>National University of Singapore</p>
              <p>Singapore</p>
              <p>jjyxmu@gmail.com</p>
            </div>
          </div>
          <!-- Add other organizers similarly -->
        </div>
      </div>
    </section>

    <!-- Program Committee Section -->
    <section id="committee" class="committee section light-background">
      <div class="container" data-aos="fade-up" data-aos-delay="100">
        <h3>Program Committee</h3>
        <ul>
          <li>Bo Han, Hong Kong Baptist University</li>
          <li>Hanwang Zhang, NTU, Singapore</li>
          <li>Lizi Liao, SMU, Singapore</li>
          <li>Kevin Lin, Microsoft Azure AI, USA</li>
          <li>Giorgos Tolias, CTU, Czech</li>
          <li>Xiangtai Li, NTU, Singapore</li>
          <li>Efstratios Gavves, UvA, Netherlands</li>
          <li>Xiatian Zhu, University of Surrey, UK</li>
          <li>Anthony Dick, The University of Adelaide, AU</li>
          <li>Zhedong Zheng, University of Macau, Macau</li>
          <li>Juncheng Li, Zhejiang University, China</li>
          <li>Yuhui Zhang, Stanford University, USA</li>
        </ul>
      </div>
    </section>

    <!-- Contact Section -->
    <section id="contact" class="contact section light-background">
      <div class="container" data-aos="fade-up" data-aos-delay="100">
        <h3>Contact</h3>
        <p>For any inquiries, please contact the organizers at mllmworkshop2025@gmail.com</p>
      </div>
    </section>

  </main>

  <footer id="footer" class="footer position-relative light-background">
    <div class="container copyright text-center mt-4">
      <p>Â© Copyright MUCG 2025 All Rights Reserved</p>
    </div>
  </footer>

  <!-- Scroll Top -->
  <a href="#" id="scroll-top" class="scroll-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Preloader -->
  <div id="preloader"></div>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>

  <!-- Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>